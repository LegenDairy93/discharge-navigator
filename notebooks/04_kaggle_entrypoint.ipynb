{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Discharge Navigator — MedGemma Impact Challenge\n\n**One notebook. One model. Fully offline clinical note extraction.**\n\nDischarge Navigator takes raw clinical notes and extracts structured, evidence-grounded discharge packets — diagnoses with ICD-10 codes, medications with dosing, follow-ups with urgency, red flags, missing information, and a patient-friendly summary. Every claim is backed by exact text spans from the source note.\n\n| | |\n|---|---|\n| **Model** | MedGemma 4B IT — Google's HAI-DEF open-weight medical model |\n| **What it does** | Single inference call → 7 structured fields, each evidence-grounded |\n| **Edge deployment** | 2.5 GB quantized (Q4_K_M), CPU-only, zero internet |\n| **Trust mechanism** | Evidence spans verified as exact substrings of the source note |\n| **Safety** | Schema validation gate rejects malformed output. Clinician reviews everything. |\n\n### How this notebook works\n\nThis notebook auto-detects your environment and picks the best available backend:\n\n| Environment | What happens |\n|---|---|\n| **Kaggle + T4 GPU** | Loads `google/medgemma-4b-it` via HuggingFace Transformers (bf16) → live extraction |\n| **Local + Ollama** | Uses `williamljx/medgemma-4b-it-Q4_K_M-GGUF` via Ollama → live extraction, CPU-only |\n| **No GPU, no Ollama** | Loads pre-computed evidence pack (46 extractions) → full demo, no model needed |\n\n**Just press Run All.**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 0 — Setup\n\nClones the [GitHub repo](https://github.com/LegenDairy93/discharge-navigator) and installs dependencies. All source code lives in `src/` — this notebook is the entrypoint, the repo is the engine.\n\n**Key source files:**\n- `src/navigator.py` — inference pipeline with retry strategy and dual prompt variants\n- `src/schemas.py` — Pydantic v2 schemas with type coercion validators\n- `src/grounding.py` — exact substring verification of evidence spans\n- `src/prompts.py` — Prompt Variant A (contract) and Variant B (strict fallback)\n- `src/hf_backend.py` — HuggingFace Transformers backend for GPU inference\n- `src/demo_app.py` — full Gradio demo (Evidence Explorer, Performance Dashboard, Edge Cases, How It Works)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Clone the repo if not already present ---\n",
    "REPO_URL = 'https://github.com/LegenDairy93/discharge-navigator.git'\n",
    "REPO_DIR = Path('discharge-navigator')\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    print('Cloning repository...')\n",
    "    subprocess.check_call(['git', 'clone', '--depth', '1', REPO_URL, str(REPO_DIR)])\n",
    "    print('Clone complete.')\n",
    "else:\n",
    "    print(f'Repo already exists at {REPO_DIR}')\n",
    "\n",
    "# Repo root IS the project root\n",
    "PROJECT_DIR = REPO_DIR\n",
    "\n",
    "# Verify project structure\n",
    "assert (PROJECT_DIR / 'src' / 'demo_app.py').exists(), 'Missing src/demo_app.py'\n",
    "assert (PROJECT_DIR / 'eval' / 'results' / 'metrics_summary.json').exists(), 'Missing eval results'\n",
    "print(f'Project root: {PROJECT_DIR.resolve()}')\n",
    "\n",
    "# --- Install dependencies ---\n",
    "deps = ['requests', 'pydantic', 'pandas', 'gradio', 'matplotlib']\n",
    "for pkg in deps:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "        print(f'Installed {pkg}')\n",
    "    else:\n",
    "        print(f'{pkg} OK')\n",
    "\n",
    "# Add project to Python path\n",
    "src_path = str(PROJECT_DIR.resolve())\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print('\\nAll dependencies ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1 — Detect Environment & Load Model\n\nThe notebook figures out what hardware is available and loads MedGemma accordingly:\n\n1. **Kaggle T4** → `google/medgemma-4b-it` at bfloat16 via HuggingFace Transformers (~8 GB VRAM)\n2. **Local CPU with Ollama** → `williamljx/medgemma-4b-it-Q4_K_M-GGUF` (2.5 GB, fully offline)\n3. **Neither available** → pre-computed evidence pack with 46 extraction results baked in\n\nBoth live backends trace to the same HAI-DEF open-weight model on Hugging Face. Same prompts, same schema validation, same grounding logic — the only difference is precision (bf16 vs Q4_K_M quantization)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "\n",
    "BACKEND = None  # 'hf', 'ollama', or None (evidence mode)\n",
    "hf_model = None\n",
    "hf_tokenizer = None\n",
    "ollama_model = None\n",
    "\n",
    "is_kaggle = os.path.exists('/kaggle')\n",
    "\n",
    "# --- Try HuggingFace (Kaggle T4 or any CUDA GPU) ---\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'GPU detected: {torch.cuda.get_device_name(0)}')\n",
    "        print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "        # Authenticate with HuggingFace\n",
    "        try:\n",
    "            if is_kaggle:\n",
    "                from kaggle_secrets import UserSecretsClient\n",
    "                hf_token = UserSecretsClient().get_secret('HF_TOKEN')\n",
    "            else:\n",
    "                hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "            if hf_token:\n",
    "                from huggingface_hub import login\n",
    "                login(token=hf_token, add_to_git_credential=False)\n",
    "                print('HuggingFace authenticated.')\n",
    "\n",
    "                from src.hf_backend import load_medgemma\n",
    "                hf_model, hf_tokenizer = load_medgemma()\n",
    "                BACKEND = 'hf'\n",
    "                print('\\nBackend: HuggingFace Transformers (GPU)')\n",
    "            else:\n",
    "                print('No HF_TOKEN found. Set it as a Kaggle Secret or env var.')\n",
    "        except Exception as e:\n",
    "            print(f'HuggingFace loading failed: {e}')\n",
    "    else:\n",
    "        print('No CUDA GPU detected.')\n",
    "except ImportError:\n",
    "    print('PyTorch not installed (expected on local without ML stack).')\n",
    "\n",
    "# --- Fallback: Try Ollama (local CPU) ---\n",
    "if BACKEND is None:\n",
    "    try:\n",
    "        from src.navigator import check_ollama, select_model\n",
    "        models = check_ollama()\n",
    "        if models:\n",
    "            ollama_model = select_model(models)\n",
    "            BACKEND = 'ollama'\n",
    "            print(f'\\nBackend: Ollama (CPU) \\u2014 model: {ollama_model}')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- Final fallback: Evidence mode ---\n",
    "if BACKEND is None:\n",
    "    print('\\nBackend: EVIDENCE MODE (pre-computed results)')\n",
    "    print('For live inference, either:')\n",
    "    print('  - Enable GPU + add HF_TOKEN secret (Kaggle)')\n",
    "    print('  - Start Ollama with MedGemma model (local)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2 — Smoke Test (Live Extraction)\n\nThis runs the **full pipeline end-to-end** on a single clinical note to prove the model is loaded and the pipeline works:\n\n`Clinical Note → MedGemma 4B → Evidence Grounding → Schema Validation`\n\nThe output shows extracted diagnoses, medications, follow-ups, and red flags — each with a grounding ratio indicating how many evidence spans were verified as exact substrings of the source note.\n\nIf no model is available, the notebook displays a pre-computed extraction (note_002 — Elevated Cardiac Enzymes case) to demonstrate what the pipeline produces."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.grounding import grounding_report\n",
    "\n",
    "if BACKEND in ('hf', 'ollama'):\n",
    "    golden_path = PROJECT_DIR / 'data' / 'golden_note.txt'\n",
    "    test_note = golden_path.read_text(encoding='utf-8')\n",
    "    print(f'Smoke test note: {len(test_note)} chars')\n",
    "    print('Running extraction...\\n')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    if BACKEND == 'hf':\n",
    "        from src.navigator import generate_packet_hf\n",
    "        packet, raw = generate_packet_hf(\n",
    "            test_note, model=hf_model, tokenizer=hf_tokenizer, return_raw=True\n",
    "        )\n",
    "    else:\n",
    "        from src.navigator import generate_packet\n",
    "        packet, raw = generate_packet(\n",
    "            test_note, model=ollama_model, return_raw=True\n",
    "        )\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    if packet is None:\n",
    "        print(f'SMOKE TEST FAILED \\u2014 could not parse output in {elapsed:.1f}s')\n",
    "        print(f'Raw output preview:\\n{raw[:500]}')\n",
    "    else:\n",
    "        d = packet.model_dump()\n",
    "        report = grounding_report(packet, test_note)\n",
    "        print(f'SMOKE TEST PASSED in {elapsed:.1f}s')\n",
    "        print(f'  Diagnoses:   {len(d[\"diagnoses\"]):>2}  (grounded: {report[\"diagnoses_grounded_ratio\"]:.0%})')\n",
    "        print(f'  Medications: {len(d[\"medications\"]):>2}  (grounded: {report[\"meds_grounded_ratio\"]:.0%})')\n",
    "        print(f'  Follow-ups:  {len(d[\"followups\"]):>2}')\n",
    "        print(f'  Red flags:   {len(d[\"red_flags\"]):>2}')\n",
    "        print(f'  Missing info:{len(d[\"missing_info\"]):>2}')\n",
    "        print(f'  Overall grounded: {report[\"overall_grounded_ratio\"]:.0%}')\n",
    "else:\n",
    "    print('Skipping live smoke test (no model available).\\n')\n",
    "    print('Pre-computed sample (note_002):')\n",
    "    sample_path = PROJECT_DIR / 'eval' / 'results' / 'samples' / 'note_002.json'\n",
    "    if sample_path.exists():\n",
    "        sample = json.loads(sample_path.read_text(encoding='utf-8'))\n",
    "        print(f'  Diagnoses:   {len(sample.get(\"diagnoses\", []))}')\n",
    "        print(f'  Medications: {len(sample.get(\"medications\", []))}')\n",
    "        print(f'  Follow-ups:  {len(sample.get(\"followups\", []))}')\n",
    "    print('\\nFull eval: 46/50 parsed, 34s median, 94% diagnosis grounding.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3 — 50-Note Evaluation Results\n\nFull evaluation across 50 MTSamples clinical notes (CC0 license). These results were generated offline and are included in the repo at `eval/results/` for reproducibility.\n\n**What was measured:**\n- **Parse rate** — did MedGemma produce valid, schema-compliant JSON?\n- **Latency** — how long per note on CPU (no GPU)?\n- **Grounding accuracy** — what percentage of cited evidence spans are verified exact substrings of the source note?\n\n**Success criteria:** ≥80% parse rate, ≥80% diagnosis grounding, median latency <120s. All three passed."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**A note on the two backends:** The smoke test above (Step 2) runs MedGemma at bfloat16 on GPU via HuggingFace Transformers. The evaluation results below were generated using the same model quantized to Q4_K_M (2.5 GB) via Ollama on CPU — the edge deployment target. Same prompts, same schema validation, same grounding logic. Minor metric differences reflect quantization trade-offs, not pipeline changes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = PROJECT_DIR / 'eval' / 'results'\n",
    "\n",
    "with open(results_dir / 'metrics_summary.json') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print('=' * 60)\n",
    "print('  DISCHARGE NAVIGATOR \\u2014 EVALUATION SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'  Model:      {summary[\"model\"]}')\n",
    "print(f'  Quant:      {summary[\"quantization\"]}')\n",
    "print(f'  Inference:  {summary[\"inference\"]}')\n",
    "print(f'  Dataset:    {summary[\"dataset\"]}')\n",
    "print(f'  Notes:      {summary[\"total_notes\"]}')\n",
    "print('-' * 60)\n",
    "print(f'  Parse rate:           {summary[\"json_valid_rate\"]:.0%}  ({summary[\"json_valid_count\"]}/{summary[\"total_notes\"]})')\n",
    "print(f'  Median latency:       {summary[\"median_latency_s\"]:.0f}s')\n",
    "print(f'  P95 latency:          {summary[\"p95_latency_s\"]:.0f}s')\n",
    "print(f'  Dx grounded (mean):   {summary[\"diagnoses_grounded_mean\"]:.0%}')\n",
    "print(f'  Meds grounded (mean): {summary[\"medications_grounded_mean\"]:.0%}')\n",
    "print(f'  Overall grounded:     {summary[\"overall_grounded_mean\"]:.0%}')\n",
    "print('-' * 60)\n",
    "for k, v in summary['success_criteria'].items():\n",
    "    status = 'PASS' if v else 'FAIL'\n",
    "    print(f'  {k}: {status}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "hist_path = results_dir / 'latency_histogram.png'\n",
    "if hist_path.exists():\n",
    "    display(Image(filename=str(hist_path), width=800))\n",
    "else:\n",
    "    print('Histogram not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4 — Interactive Demo\n\nLaunches the full Gradio application with four tabs:\n\n| Tab | What it shows |\n|-----|---------------|\n| **Evidence Explorer** | Load any note → see extractions with highlighted evidence spans. Filter by diagnoses, medications, or follow-ups. View patient summary, red flags, and missing information with severity tiers. |\n| **Performance Dashboard** | Quantitative metrics with pass/fail badges from the 50-note evaluation. |\n| **Edge Cases** | The 4 notes that failed — what went wrong and why it's safe (schema gate caught everything). |\n| **How It Works** | Glossary, pipeline walkthrough, and confidence/grounding explainer for clinicians. |\n\nThe demo works in all three modes — live inference produces fresh extractions, evidence mode uses the pre-computed pack. A public share link is generated automatically on Kaggle.\n\n**The product is not the extraction — it's the verification interface.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.demo_app import build_app, LAUNCH_KWARGS\n\napp = build_app()\n\napp.launch(\n    share=True,\n    server_name='0.0.0.0',\n    server_port=7860,\n    **LAUNCH_KWARGS,\n)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}