{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discharge Navigator — Kaggle Entrypoint\n",
    "\n",
    "**One-click reproducibility.** Run All → repo cloned → model loaded → live extraction → evidence displayed.\n",
    "\n",
    "| Component | Detail |\n",
    "|-----------|--------|\n",
    "| Model | MedGemma 4B IT |\n",
    "| Inference | HuggingFace Transformers (Kaggle T4) or Ollama (local CPU) |\n",
    "| Dataset | MTSamples (CC0, Kaggle) |\n",
    "| Pipeline | Extract → Verify → Validate → Clinician Review |\n",
    "\n",
    "**Auto-detects environment:**\n",
    "- Kaggle + T4 GPU → live inference via HuggingFace Transformers\n",
    "- Local + Ollama → live inference via Ollama (CPU, offline)\n",
    "- No GPU, no Ollama → pre-computed evidence pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 — Clone Repo + Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess, sys, os\nfrom pathlib import Path\n\n# --- Clone the repo if not already present ---\nREPO_URL = 'https://github.com/LegenDairy93/discharge-navigator.git'\nREPO_DIR = Path('discharge-navigator')\n\nif not REPO_DIR.exists():\n    print('Cloning repository...')\n    subprocess.check_call(['git', 'clone', '--depth', '1', REPO_URL, str(REPO_DIR)])\n    print('Clone complete.')\nelse:\n    print(f'Repo already exists at {REPO_DIR}')\n\n# Repo root IS the project root\nPROJECT_DIR = REPO_DIR\n\n# Verify project structure\nassert (PROJECT_DIR / 'src' / 'demo_app.py').exists(), 'Missing src/demo_app.py'\nassert (PROJECT_DIR / 'eval' / 'results' / 'metrics_summary.json').exists(), 'Missing eval results'\nprint(f'Project root: {PROJECT_DIR.resolve()}')\n\n# --- Install dependencies ---\ndeps = ['requests', 'pydantic', 'pandas', 'gradio', 'matplotlib']\nfor pkg in deps:\n    try:\n        __import__(pkg)\n    except ImportError:\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n        print(f'Installed {pkg}')\n    else:\n        print(f'{pkg} OK')\n\n# Add project to Python path\nsrc_path = str(PROJECT_DIR.resolve())\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\n\nprint('\\nAll dependencies ready.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 — Detect Environment + Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json, time\n\nBACKEND = None  # 'hf', 'ollama', or None (evidence mode)\nhf_model = None\nhf_tokenizer = None\nollama_model = None\n\nis_kaggle = os.path.exists('/kaggle')\n\n# --- Try HuggingFace (Kaggle T4 or any CUDA GPU) ---\ntry:\n    import torch\n    if torch.cuda.is_available():\n        print(f'GPU detected: {torch.cuda.get_device_name(0)}')\n        print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n\n        # Authenticate with HuggingFace\n        try:\n            if is_kaggle:\n                from kaggle_secrets import UserSecretsClient\n                hf_token = UserSecretsClient().get_secret('HF_TOKEN')\n            else:\n                hf_token = os.environ.get('HF_TOKEN')\n\n            if hf_token:\n                from huggingface_hub import login\n                login(token=hf_token, add_to_git_credential=False)\n                print('HuggingFace authenticated.')\n\n                from src.hf_backend import load_medgemma\n                hf_model, hf_tokenizer = load_medgemma()\n                BACKEND = 'hf'\n                print('\\nBackend: HuggingFace Transformers (GPU)')\n            else:\n                print('No HF_TOKEN found. Set it as a Kaggle Secret or env var.')\n        except Exception as e:\n            print(f'HuggingFace loading failed: {e}')\n    else:\n        print('No CUDA GPU detected.')\nexcept ImportError:\n    print('PyTorch not installed (expected on local without ML stack).')\n\n# --- Fallback: Try Ollama (local CPU) ---\nif BACKEND is None:\n    try:\n        from src.navigator import check_ollama, select_model\n        models = check_ollama()\n        if models:\n            ollama_model = select_model(models)\n            BACKEND = 'ollama'\n            print(f'\\nBackend: Ollama (CPU) \\u2014 model: {ollama_model}')\n    except Exception:\n        pass\n\n# --- Final fallback: Evidence mode ---\nif BACKEND is None:\n    print('\\nBackend: EVIDENCE MODE (pre-computed results)')\n    print('For live inference, either:')\n    print('  - Enable GPU + add HF_TOKEN secret (Kaggle)')\n    print('  - Start Ollama with MedGemma model (local)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Smoke Test (1 Note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.grounding import grounding_report\n",
    "\n",
    "if BACKEND in ('hf', 'ollama'):\n",
    "    golden_path = PROJECT_DIR / 'data' / 'golden_note.txt'\n",
    "    test_note = golden_path.read_text(encoding='utf-8')\n",
    "    print(f'Smoke test note: {len(test_note)} chars')\n",
    "    print('Running extraction...\\n')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    if BACKEND == 'hf':\n",
    "        from src.navigator import generate_packet_hf\n",
    "        packet, raw = generate_packet_hf(\n",
    "            test_note, model=hf_model, tokenizer=hf_tokenizer, return_raw=True\n",
    "        )\n",
    "    else:\n",
    "        from src.navigator import generate_packet\n",
    "        packet, raw = generate_packet(\n",
    "            test_note, model=ollama_model, return_raw=True\n",
    "        )\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    if packet is None:\n",
    "        print(f'SMOKE TEST FAILED \\u2014 could not parse output in {elapsed:.1f}s')\n",
    "        print(f'Raw output preview:\\n{raw[:500]}')\n",
    "    else:\n",
    "        d = packet.model_dump()\n",
    "        report = grounding_report(packet, test_note)\n",
    "        print(f'SMOKE TEST PASSED in {elapsed:.1f}s')\n",
    "        print(f'  Diagnoses:   {len(d[\"diagnoses\"]):>2}  (grounded: {report[\"diagnoses_grounded_ratio\"]:.0%})')\n",
    "        print(f'  Medications: {len(d[\"medications\"]):>2}  (grounded: {report[\"meds_grounded_ratio\"]:.0%})')\n",
    "        print(f'  Follow-ups:  {len(d[\"followups\"]):>2}')\n",
    "        print(f'  Red flags:   {len(d[\"red_flags\"]):>2}')\n",
    "        print(f'  Missing info:{len(d[\"missing_info\"]):>2}')\n",
    "        print(f'  Overall grounded: {report[\"overall_grounded_ratio\"]:.0%}')\n",
    "else:\n",
    "    print('Skipping live smoke test (no model available).\\n')\n",
    "    print('Pre-computed sample (note_002):')\n",
    "    sample_path = PROJECT_DIR / 'eval' / 'results' / 'samples' / 'note_002.json'\n",
    "    if sample_path.exists():\n",
    "        sample = json.loads(sample_path.read_text(encoding='utf-8'))\n",
    "        print(f'  Diagnoses:   {len(sample.get(\"diagnoses\", []))}')\n",
    "        print(f'  Medications: {len(sample.get(\"medications\", []))}')\n",
    "        print(f'  Follow-ups:  {len(sample.get(\"followups\", []))}')\n",
    "    print('\\nFull eval: 46/50 parsed, 34s median, 94% diagnosis grounding.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 — Evidence Pack Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Note on inference backends:** The live smoke test above uses MedGemma 4B at bfloat16 precision on GPU via HuggingFace Transformers. The evidence pack below was generated offline using the same model quantized to Q4_K_M (2.5 GB) via Ollama on CPU — the edge deployment target. Same prompts, same schema validation, same grounding logic. Minor metric differences reflect quantization, not pipeline changes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = PROJECT_DIR / 'eval' / 'results'\n",
    "\n",
    "with open(results_dir / 'metrics_summary.json') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print('=' * 60)\n",
    "print('  DISCHARGE NAVIGATOR \\u2014 EVALUATION SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'  Model:      {summary[\"model\"]}')\n",
    "print(f'  Quant:      {summary[\"quantization\"]}')\n",
    "print(f'  Inference:  {summary[\"inference\"]}')\n",
    "print(f'  Dataset:    {summary[\"dataset\"]}')\n",
    "print(f'  Notes:      {summary[\"total_notes\"]}')\n",
    "print('-' * 60)\n",
    "print(f'  Parse rate:           {summary[\"json_valid_rate\"]:.0%}  ({summary[\"json_valid_count\"]}/{summary[\"total_notes\"]})')\n",
    "print(f'  Median latency:       {summary[\"median_latency_s\"]:.0f}s')\n",
    "print(f'  P95 latency:          {summary[\"p95_latency_s\"]:.0f}s')\n",
    "print(f'  Dx grounded (mean):   {summary[\"diagnoses_grounded_mean\"]:.0%}')\n",
    "print(f'  Meds grounded (mean): {summary[\"medications_grounded_mean\"]:.0%}')\n",
    "print(f'  Overall grounded:     {summary[\"overall_grounded_mean\"]:.0%}')\n",
    "print('-' * 60)\n",
    "for k, v in summary['success_criteria'].items():\n",
    "    status = 'PASS' if v else 'FAIL'\n",
    "    print(f'  {k}: {status}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "hist_path = results_dir / 'latency_histogram.png'\n",
    "if hist_path.exists():\n",
    "    display(Image(filename=str(hist_path), width=800))\n",
    "else:\n",
    "    print('Histogram not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 — Launch Trust Surface Demo\n",
    "\n",
    "Interactive demo with traceability panel, reliability board, and failure analysis.\n",
    "On Kaggle, a public share link will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.demo_app import build_app\n\napp = build_app()\n\napp.launch(\n    share=True,\n    server_name='0.0.0.0',\n    server_port=7860,\n)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}